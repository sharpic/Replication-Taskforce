% Replication and Independent Verification
% Laleh Behjat, Bruce Childers, Eric Eide, Grigori Fursin, Yolanda Gil, Simon Harper, Alex K. Jones, Shriram Krishnamurthi, Jimmy Lin, Patrick Madden, and Catherine McGeoch.
% 16 October 2015

Replication and independent verification of results are commonplace in many scientific fields. These practices help avoid misunderstandings, provide greater confidence in reported results, and enable the detection of both unintentional error and deliberate fraud. By comparison with fields such as physics, chemistry, and medicine, computer science is clearly lagging in this regard; replication of experiments is rare, and activities such as paper correction or retraction are virtually unheard of.

This document corrects this situation by setting out the view of the ACM SGB in relation to replication and independent verification of results. Therefore SIG focused views and requirements can be adopted into the central ACM publications process.

## Introduction

The scientific method is based on empirical data and the replicability of those experiments used to collect this data by third parties. However, in Computer Science the ability to replicate experiments is limited because there are few peer reviewed routes for experimental replication.

Our limited ability to replicate experiments -- due to a lack of public data -- means that are domains do not strictly follow the scientific method, and as such are sometimes seen as less rigourous than other scientific domains. Indeed, we may go further and suggest that:

1. Methodical data collection and experimentation is variable because the data may never been seen by a third party;

1. Experiments to confirm the science are not repeated as the data and experimentation methodology is not available, and without a publication route there is no value in doing so; and 

1. Data collection and accurate experimentation is not seen as being important for citation and publication of the data alone, and is seen more as a second class citizen to an system-based technical paper.

Other sciences (Physics, Ecology / Biology, Medicine etc.) recognise the value of their data as it costs (often) enormous amounts of time, effort, and money to acquire. As a discipline we need to plug this gap. In this case, the charge of the task force is to develop proposals on how ACM can bring current replication and verification practices in line with the rest of the scientific community. Specifically, the following questions are of interest.

1. Across the SIGs, what mechanisms are in place to enable replication and verification efforts? Are they sufficient to catch errors or deliberate fraud (and have they in fact done so?). Can these processes be improved or encouraged in some way?

1. What are reasonable expectations from authors, with regards to enabling replication and verification? How can these expectations be made compatible with the review process in conferences, symposia, journals? Should there be different expectations during review, and after paper acceptance?

1. If published work fails to be replicated, appears to contain significant errors, or even appears to be fraudulent -- what mechanisms should be in place to investigate the matter? Who would such concerns be communicated to? Who would perform investigation, and decide the matter? And if errors, deception, or fraud are confirmed â€” what should the ACM, as a publisher, do? Should there be corrections, annotations noting the errors, paper retraction?

The SIGs, with elected leaders representing the various research communities of computer science, are a logical group to engage in setting policy. The SIG leaders will be familiar with typical practices within their areas of expertise. With control of many of the major publishing venues, this group also has the ability to enact change.

# Experiences
{>>START: Cut and Paste this Block<<}
## [Change to your SIG/Organisation]

### Systems used

### Process Followed

### Changes Made

### Other Comments
{>>END: Cut and Paste this Block<<}










## Afterward
The [ACM](http://www.acm.org) [SGB](http://www.acm.org/sigs/sgb/) Replication Taskforce was initiated at the request of the SGB Chair [Patrick Madden](http://dl.acm.org/author_page.cfm?id=81100092346) in September 2015. SIG Development Advisor [Simon Harper](http://dl.acm.org/author_page.cfm?id=81100139139) was appointed Taskforce Chair to take this forward as well as integrating it into the 'DL-Technology: Data, Software, & Reproducibility' project.

The charge of the task force was to develop proposals on how the ACM can bring current replication and verification practices in line with the rest of the scientific community. Given the range of research topics and problems considered, there was no one-size-fits-all policy. With this in mind this group was open to all voices across the SGB and composed SGB members and other invited experts. The document is also intended to evolve (hence the versioning here, and the versioned releases) and be transparent. By engaging many SIGs in this effort, a degree of consistency was achieved across the ACM.

### Contributors / Authors
- Behjat, Laleh
- Childers, Bruce
- Eide, Eric (SIGOPS)
- Fursin, Grigori
- Gil, Yolanda (SIGAI)
- Harper, Simon (Taskforce Chair)
- Jones, Alex K.
- Krishnamurthic, Shriram (SIGPLAN)
- Lin, Jimmy (SIGIR)
- Madden, Patrick (SGB Chair)
- McGeoch, Catherine

### Colophon
This document is created in standard '[Markdown](http://daringfireball.net/projects/markdown/)' with [pandoc](http://pandoc.org/) [citation additions](http://pandoc.org/README.html#citations). [Critic Markup](http://criticmarkup.com/) is used for [comments](http://criticmarkup.com/users-guide.php). The output format is as yet unknown and so a basic markdown document which can be easily converted to many formats (via pandoc or mmd) is most appropriate. Further, version control via Git is easier using basic text formats.  
